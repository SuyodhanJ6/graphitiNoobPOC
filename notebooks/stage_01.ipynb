{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii\n"
     ]
    }
   ],
   "source": [
    "print(\"Hii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Neo4j connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19428/3585152814.py:38: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Neo4j connection successful!\n",
      "Number of nodes in database: 0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Neo4j credentials from environment variables\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "def test_direct_connection():\n",
    "    \"\"\"Test direct Neo4j connection using the Python driver\"\"\"\n",
    "    try:\n",
    "        # Create a driver instance\n",
    "        driver = GraphDatabase.driver(\n",
    "            NEO4J_URI,\n",
    "            auth=(NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "        )\n",
    "        \n",
    "        # Verify connectivity\n",
    "        driver.verify_connectivity()\n",
    "        print(\"Direct Neo4j connection successful!\")\n",
    "        \n",
    "        return driver\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "    \n",
    "def test_langchain_connection():\n",
    "    \"\"\"Test Neo4j connection using LangChain\"\"\"\n",
    "    try:\n",
    "        # Initialize Neo4jGraph with credentials\n",
    "        graph = Neo4jGraph(\n",
    "            url=NEO4J_URI,\n",
    "            username=NEO4J_USERNAME,\n",
    "            password=NEO4J_PASSWORD\n",
    "        )\n",
    "        \n",
    "        # Test a simple query\n",
    "        result = graph.query(\"MATCH (n) RETURN count(n) as count\")\n",
    "        print(\"LangChain Neo4j connection successful!\")\n",
    "        print(f\"Number of nodes in database: {result[0]['count']}\")\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting via LangChain: {e}\")\n",
    "        return None\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# Test both connection methods\n",
    "direct_driver = test_direct_connection()\n",
    "langchain_graph = test_langchain_connection()\n",
    "\n",
    "# Clean up direct driver connection if it was successful\n",
    "if direct_driver:\n",
    "    direct_driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sseclient\n",
    "\n",
    "# MCP server URL\n",
    "MCP_URL = \"http://localhost:8080\"  # Change to your MCP server address\n",
    "\n",
    "def upload_markdown_file_mcp(file_path, group_id=None):\n",
    "    \"\"\"\n",
    "    Upload a Markdown file to Graphiti MCP server\n",
    "    \"\"\"\n",
    "    # Read the Markdown file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Create MCP message\n",
    "    mcp_message = {\n",
    "        \"type\": \"episode\",\n",
    "        \"operation\": \"create\",\n",
    "        \"data\": {\n",
    "            \"content\": content,\n",
    "            \"content_type\": \"text/markdown\",\n",
    "            \"source\": os.path.basename(file_path)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add group if specified\n",
    "    if group_id:\n",
    "        mcp_message[\"data\"][\"group_id\"] = group_id\n",
    "    \n",
    "    # Send to MCP server\n",
    "    response = requests.post(\n",
    "        f\"{MCP_URL}/v1/messages\",\n",
    "        json=mcp_message,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error uploading file: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "    result = response.json()\n",
    "    episode_id = result.get(\"data\", {}).get(\"id\")\n",
    "    print(f\"Episode created with ID: {episode_id}\")\n",
    "    return episode_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # System Prompt for Graphiti-Enabled Agent\n",
    "\n",
    "# You are an AI assistant with access to Graphiti's knowledge graph capabilities. You can maintain context and knowledge across conversations using the following tools:\n",
    "\n",
    "# ## Your Available Tools:\n",
    "\n",
    "# 1. `add_episode`: Store information in the knowledge graph\n",
    "#    - Use this to save conversations, requirements, or any important information\n",
    "#    - Example: Recording a customer meeting, technical requirements, or project details\n",
    "#    ```python\n",
    "#    add_episode(\n",
    "#        name=\"Meeting with Client X\",\n",
    "#        episode_body=\"Client discussed requirements...\",\n",
    "#        source=\"text\"\n",
    "#    )\n",
    "#    ```\n",
    "\n",
    "# 2. `search_nodes`: Find relevant information\n",
    "#    - Use this to find previously stored information\n",
    "#    - Example: Looking up past conversations or requirements\n",
    "#    ```python\n",
    "#    search_nodes(\n",
    "#        query=\"security requirements for Project X\"\n",
    "#    )\n",
    "#    ```\n",
    "\n",
    "# 3. `search_facts`: Find relationships between entities\n",
    "#    - Use this to understand connections between different pieces of information\n",
    "#    - Example: Finding connections between requirements and projects\n",
    "\n",
    "# 4. `get_episodes`: Retrieve recent conversations/information\n",
    "#    - Use this to review recent interactions\n",
    "#    - Example: Getting last 5 conversations about a specific topic\n",
    "\n",
    "# ## Your Responsibilities:\n",
    "\n",
    "# 1. Information Storage:\n",
    "#    - Store important information from conversations\n",
    "#    - Record requirements, preferences, and procedures\n",
    "#    - Maintain context across interactions\n",
    "\n",
    "# 2. Information Retrieval:\n",
    "#    - Search for relevant past information when needed\n",
    "#    - Connect related pieces of information\n",
    "#    - Provide context from previous interactions\n",
    "\n",
    "# 3. Knowledge Organization:\n",
    "#    - Use appropriate group_ids to organize related information\n",
    "#    - Structure information clearly and consistently\n",
    "#    - Link related pieces of information\n",
    "\n",
    "# ## When to Use Each Tool:\n",
    "\n",
    "# USE `add_episode` WHEN:\n",
    "# - Recording new conversations\n",
    "# - Storing requirements or specifications\n",
    "# - Saving important decisions or outcomes\n",
    "# - Documenting technical details\n",
    "\n",
    "# USE `search_nodes` WHEN:\n",
    "# - Looking for specific information\n",
    "# - Finding relevant past conversations\n",
    "# - Checking existing requirements\n",
    "# - Verifying previous decisions\n",
    "\n",
    "# USE `search_facts` WHEN:\n",
    "# - Understanding relationships between entities\n",
    "# - Finding connections between requirements\n",
    "# - Tracing decision dependencies\n",
    "# - Mapping project relationships\n",
    "\n",
    "# USE `get_episodes` WHEN:\n",
    "# - Reviewing recent conversations\n",
    "# - Checking latest updates\n",
    "# - Following up on previous discussions\n",
    "\n",
    "# ## Information Structure Guidelines:\n",
    "\n",
    "# When adding episodes, structure information like this:\n",
    "\n",
    "# FOR CONVERSATIONS:\n",
    "# ```text\n",
    "# Topic: [Main Topic]\n",
    "# Participants: [Who was involved]\n",
    "# Key Points:\n",
    "# - [Point 1]\n",
    "# - [Point 2]\n",
    "# Decisions Made:\n",
    "# - [Decision 1]\n",
    "# - [Decision 2]\n",
    "# Next Steps:\n",
    "# - [Action item 1]\n",
    "# - [Action item 2]\n",
    "# ```\n",
    "\n",
    "# FOR REQUIREMENTS:\n",
    "# ```text\n",
    "# Requirement Type: [Technical/Business/Security]\n",
    "# Description: [Clear description]\n",
    "# Priority: [High/Medium/Low]\n",
    "# Dependencies: [Any dependencies]\n",
    "# Stakeholders: [Who is involved]\n",
    "# ```\n",
    "\n",
    "# FOR TECHNICAL DOCUMENTATION:\n",
    "# ```text\n",
    "# Component: [Name]\n",
    "# Purpose: [Description]\n",
    "# Technical Details:\n",
    "# - [Detail 1]\n",
    "# - [Detail 2]\n",
    "# Dependencies: [List]\n",
    "# ```\n",
    "\n",
    "# ## Best Practices:\n",
    "\n",
    "# 1. Always add context when storing information\n",
    "# 2. Use clear, descriptive names for episodes\n",
    "# 3. Include relevant metadata (dates, participants, etc.)\n",
    "# 4. Link related information using consistent group_ids\n",
    "# 5. Structure information for easy retrieval\n",
    "# 6. Include key terms for better searchability\n",
    "\n",
    "# ## Response Format:\n",
    "\n",
    "# When responding to queries:\n",
    "# 1. First, search for relevant context using appropriate tools\n",
    "# 2. Combine information from multiple sources if needed\n",
    "# 3. Present information clearly and structured\n",
    "# 4. Store new important information from the interaction\n",
    "# 5. Link new information to existing knowledge\n",
    "\n",
    "# Remember: Your goal is to maintain a coherent knowledge graph that helps provide better context and more informed responses across conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 221\u001b[39m\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m processor.close()\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import argparse\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dotenv import load_dotenv\n",
    "import mimetypes\n",
    "import pathlib\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain MCP adapter imports\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Agent for processing documents and storing knowledge using Graphiti\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4\"):\n",
    "        self.model_name = model_name\n",
    "        self.llm = None\n",
    "        self.agent = None\n",
    "        self.mcp_client = None\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI assistant with access to both document processing and knowledge graph capabilities.\n",
    "        \n",
    "        Available Tools:\n",
    "        1. Document Processing (MarkItDown):\n",
    "           - Process various document formats (PDF, PPT, Word, Excel, etc.)\n",
    "           - Extract text from images using OCR\n",
    "           - Transcribe audio files\n",
    "           - Parse structured formats (CSV, JSON, XML)\n",
    "           - Process web content and YouTube URLs\n",
    "        \n",
    "        2. Knowledge Graph (Graphiti):\n",
    "           - add_episode: Store processed information\n",
    "           - search_nodes: Find relevant information\n",
    "           - search_facts: Find relationships\n",
    "           - get_episodes: Retrieve conversation history\n",
    "        \n",
    "        Your responsibilities:\n",
    "        1. Process uploaded documents effectively\n",
    "        2. Extract meaningful information\n",
    "        3. Store processed content in the knowledge graph\n",
    "        4. Maintain relationships between documents and information\n",
    "        5. Provide structured access to stored knowledge\n",
    "        \"\"\"\n",
    "    \n",
    "    async def setup(self):\n",
    "        \"\"\"Set up the agent with both MarkItDown and Graphiti tools.\"\"\"\n",
    "        self.llm = ChatOpenAI(model=self.model_name, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        try:\n",
    "            # Connect to both MCP servers\n",
    "            self.mcp_client = MultiServerMCPClient(\n",
    "                {\n",
    "                    \"graphiti\": {\n",
    "                        \"url\": \"http://localhost:8000/sse\",\n",
    "                        \"transport\": \"sse\",\n",
    "                    },\n",
    "                    \"markitdown\": {\n",
    "                        \"url\": \"http://127.0.0.1:3001/sse\",\n",
    "                        \"transport\": \"sse\",\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            await self.mcp_client.__aenter__()\n",
    "            mcp_tools = self.mcp_client.get_tools()\n",
    "            print(f\"Loaded {len(mcp_tools)} tools from MCP servers\")\n",
    "            \n",
    "            self.agent = create_react_agent(\n",
    "                self.llm,\n",
    "                mcp_tools,\n",
    "                prompt=self.system_prompt\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to MCP servers: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a document and store its information.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document to process\n",
    "            \n",
    "        Returns:\n",
    "            Processing and storage results\n",
    "        \"\"\"\n",
    "        if not self.agent:\n",
    "            await self.setup()\n",
    "        \n",
    "        try:\n",
    "            # Get file type\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            file_ext = pathlib.Path(file_path).suffix.lower()\n",
    "            \n",
    "            process_prompt = f\"\"\"\n",
    "            Please process this document and store its information:\n",
    "            \n",
    "            File: {file_path}\n",
    "            Type: {mime_type or file_ext}\n",
    "            \n",
    "            1. Use appropriate MarkItDown tools to process the document\n",
    "            2. Extract key information and insights\n",
    "            3. Store the processed information in the knowledge graph using add_episode\n",
    "            4. Create appropriate relationships using search_facts if relevant\n",
    "            \n",
    "            Please provide a summary of what was processed and stored.\n",
    "            \"\"\"\n",
    "            \n",
    "            result = await self.agent.ainvoke({\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=process_prompt)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Error processing document: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    async def search_documents(self, query: str, doc_types: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Search for information across processed documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            doc_types: Optional list of document types to search within\n",
    "            \n",
    "        Returns:\n",
    "            Search results\n",
    "        \"\"\"\n",
    "        if not self.agent:\n",
    "            await self.setup()\n",
    "        \n",
    "        try:\n",
    "            type_filter = f\" within {', '.join(doc_types)}\" if doc_types else \"\"\n",
    "            search_prompt = f\"\"\"\n",
    "            Please search for information{type_filter} about:\n",
    "            \n",
    "            {query}\n",
    "            \n",
    "            1. Use search_nodes to find relevant documents and information\n",
    "            2. Use search_facts to find relationships between documents\n",
    "            3. Compile the results in a clear, structured format\n",
    "            \"\"\"\n",
    "            \n",
    "            result = await self.agent.ainvoke({\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=search_prompt)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Error searching documents: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.mcp_client:\n",
    "            await self.mcp_client.__aexit__(None, None, None)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run the document processor.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Document Processor with Knowledge Graph\")\n",
    "    parser.add_argument(\"--action\", choices=[\"process\", \"search\"], required=True,\n",
    "                      help=\"Action to perform (process or search)\")\n",
    "    parser.add_argument(\"--file\", help=\"Path to file to process\")\n",
    "    parser.add_argument(\"--query\", help=\"Search query\")\n",
    "    parser.add_argument(\"--doc-types\", nargs=\"+\", help=\"Document types to search within\")\n",
    "    parser.add_argument(\"--model\", default=\"gpt-4\", help=\"LLM model to use\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"ERROR: OPENAI_API_KEY not found in environment variables\")\n",
    "        print(\"Please set it in your .env file\")\n",
    "        return\n",
    "    \n",
    "    processor = DocumentProcessor(model_name=args.model)\n",
    "    \n",
    "    try:\n",
    "        if args.action == \"process\":\n",
    "            if not args.file:\n",
    "                print(\"ERROR: --file is required for process action\")\n",
    "                return\n",
    "            result = await processor.process_document(args.file)\n",
    "        else:  # search\n",
    "            if not args.query:\n",
    "                print(\"ERROR: --query is required for search action\")\n",
    "                return\n",
    "            result = await processor.search_documents(args.query, args.doc_types)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        # Display the result\n",
    "        ai_messages = [msg for msg in result[\"messages\"] if isinstance(msg, AIMessage)]\n",
    "        if ai_messages:\n",
    "            print(\"\\n===== PROCESSING RESULT =====\")\n",
    "            print(ai_messages[-1].content)\n",
    "        else:\n",
    "            print(\"No result was generated\")\n",
    "    \n",
    "    finally:\n",
    "        await processor.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
